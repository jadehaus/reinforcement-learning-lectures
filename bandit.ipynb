{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화 학습 환경 복습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화 학습 문제를 직접 풀어낼 정책 정의\n",
    "\n",
    "강화 학습에서는 어떤 함수를 학습하고자 하는 걸까요? 에이전트 안에는 상태 관측값(입력)을 받고 그것을 앞으로 취해야 할 최적의 행동(출력)에 매핑하는 함수가 있습니다. 예를 들어, 미로 속 에이전트의 현재 상태가 $(2, 3)$ 좌표라면, 에이전트 안의 함수는 이 입력값을 \"오른쪽으로 이동\"이라는 출력값에 매핑하는 것이 될 수 있습니다. 이 함수를 $\\pi$라고 한다면, 아래와 같이 수식으로 쓸 수 있습니다.\n",
    "$$\n",
    "\\pi((2, 3)) = \\text{\"오른쪽으로 이동\"}\n",
    "$$\n",
    "강화 학습 용어로 이 함수를 정책(policy)이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    x_position = state[0]\n",
    "    x_velocity = state[1]\n",
    "\n",
    "    if x_velocity > 0:\n",
    "        action = 2\n",
    "    elif x_velocity < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        if x_position > -0.6:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 2\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화 학습이 돌아가는 환경의 코드 복습\n",
    "\n",
    "1. 인공지능 모델은 환경의 현재 상태(state)를 관찰할 수 있습니다. 미로 찾기 문제에서 환경의 현재 상태란 미로 속 현재 위치를 의미합니다. 예를 들어, 모델이 미로의 $(2, 3)$ 좌표에 있다면, 이 좌표는 현재 상태를 나타냅니다.\n",
    "\n",
    "2. 인공지능 모델은 관찰된 상태로부터 앞으로 취할 행동(action)을 결정합니다. 양갈래 길 중에서 어디로 갈지 결정하는 것 등이 그 예시가 될 수 있습니다.\n",
    "\n",
    "3. 환경은 상태를 변경(transition)시키고 그 행동에 대한 보상(reward)을 생성합니다. 인공지능 모델은 그 상태와 보상을 다 받습니다. 미로 찾기 문제에서 환경의 변화란 인공지능 모델의 (앞선 결정에 따른) 미로 속 위치 변화를 의미합니다. 예를 들어, '오른쪽으로 이동' 행동을 취하면, 에이전트의 위치 좌표가 $(2, 3)$에서 $(2, 4)$로 바뀔 수 있습니다. 보상은 출구를 찾았을 때 주어지는 경품이나 막다른 길에 도달했을 때 받는 페널티 등을 생각해 볼 수 있습니다.\n",
    "\n",
    "4.  이 새로운 정보(환경의 변화와 이에 따른 보상)를 사용하여 인공지능은 그런 행동이 좋아 그걸 반복해야 하는지, 또는 좋지 않아 회피해야 하는지 결정할 수 있습니다. 완료될 때까지 (done) 이 관측-행동-보상 사이클은 계속됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "state, _ = env.reset()\n",
    "print(\"Initial state:\", state)\n",
    "\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action = policy(state)\n",
    "    print(\"Chose action:\", action)\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(\"New state:\", state)\n",
    "    print(\"Reward:\", reward)\n",
    "\n",
    "print(\"Final state:\", state)\n",
    "print(\"Total reward:\", total_reward)\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 직접 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = gym.spaces.Discrete(7, start=-3)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.num_steps = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        state = 0\n",
    "        return state\n",
    "        \n",
    "    def step(self, action):\n",
    "        self.num_steps += 1\n",
    "\n",
    "        if action == 0:\n",
    "            next_state = state - 1\n",
    "        else:\n",
    "            next_state = state + 1\n",
    "        \n",
    "        if next_state > 3:\n",
    "            next_state = 3\n",
    "        elif next_state < -3:\n",
    "            next_state = -3\n",
    "        \n",
    "        reward = {\n",
    "            -3: 1,\n",
    "            -2: 1,\n",
    "            -1: 1,\n",
    "            0: 0,\n",
    "            1: -1,\n",
    "            2: -1,\n",
    "            3: 10\n",
    "        }[next_state]\n",
    "\n",
    "        done = self.num_steps >= 3\n",
    "        return next_state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentMDP(gym.Env):\n",
    "    def __init__(self):\n",
    "        # 0: 수업, 1: 야자, 2: 집, 3: 잘침, 4: 못침\n",
    "        self.observation_space = gym.spaces.Discrete(5)\n",
    "\n",
    "        # 0: 공부, 1: 딴짓, 2: 땡땡이, 3: 쇼츠보기, 4: 벼락치기, 5: 수면\n",
    "        self.action_space = gym.spaces.Discrete(6)\n",
    "        \n",
    "    def reset(self):\n",
    "        state = ???\n",
    "        return state\n",
    "        \n",
    "    def step(self, action):\n",
    "        state = ???\n",
    "        reward = ???\n",
    "        done = ???\n",
    "        return state, reward, done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 슬롯머신 정복하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 슬롯머신 환경 구현하기\n",
    "\n",
    "각각의 밴딧 $i = 1, \\cdots, n$에 대해, 먼저 랜덤하게 실제 가치 $\\mu_{i} \\sim N(0, 1)$를 평균 0 분산 1인 표준정규분포에서 추출해 정해줍니다. 그리고 $i$번 째 밴딧을 고를 경우, 받을 수 있는 보상은 다음과 같이 정해줍니다:\n",
    "$$\n",
    "r_{t} = \\mu_{i} + \\varepsilon \\quad \\text{where} \\quad \\varepsilon \\sim N(0, 1)\n",
    "$$\n",
    "즉, $i$번 째 밴딧을 골랐을 때의 보상은, 그 밴딧의 실제 가치 $\\mu_{i}$에 랜덤한 표준 정규 노이즈 $\\varepsilon \\sim N(0, 1)$을 더한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class BanditEnv(gym.Env):\n",
    "    def __init__(self, num_bandits):\n",
    "        self.num_bandits = num_bandits\n",
    "        self.action_space = ???\n",
    "        self.observation_space = ???\n",
    "        \n",
    "    def reset(self):\n",
    "        state = ???\n",
    "        ??????????\n",
    "        return ???\n",
    "        \n",
    "    def step(self, action):\n",
    "        state = ???\n",
    "        reward = ???\n",
    "        done = ???\n",
    "        return state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 슬롯머신 풀어보기\n",
    "\n",
    "직접 위 환경과 상호작용하면서, 각 슬롯머신의 보상을 추정해보세요. 그리고 Python을 이용해 위 문제에서 $\\epsilon=0.1$과 $\\epsilon=0.01$에 대해 각각 $\\epsilon$-greedy 방법을 적용해 보세요. (초기값 $Q_{1}(i)$는 모두 0으로 설정.) 또한 $Q_{1}(i) = 5$인 완전 탐욕적인 알고리즘 ($\\epsilon=0$) 또한 Python으로 구현하여 그 결과를 비교해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, num_bandits, epsilon):\n",
    "        self.num_bandits = num_bandits\n",
    "        self.epsilon = epsilon\n",
    "        ????\n",
    "        \n",
    "    def select_action(self):\n",
    "        action = ????\n",
    "        return action\n",
    "    \n",
    "\n",
    "env = BanditEnv(10)\n",
    "policy = EpsilonGreedyPolicy(10, 0.1)\n",
    "\n",
    "state = env.reset()\n",
    "total_reward = 0\n",
    "for t in range(100):\n",
    "    action = policy.select_action()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    print(\"Action:\", action, \"Reward:\", reward)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Total reward:\", total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
