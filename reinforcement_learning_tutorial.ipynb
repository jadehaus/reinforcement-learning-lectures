{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2BsaXtWqpAp"
      },
      "source": [
        "# 강화 학습 맛보기\n",
        "\n",
        "강화 학습 수업에 오신 여러분을 진심으로 환영합니다!\n",
        "\n",
        "여러분이 앞으로 1년 동안 배우게 될 강화 학습은 인공지능 분야 중에서도 가장 독특하고 유용한 기술입니다.\n",
        "아마 다른 어떤 수업에서도 경험하지 못할 재미있고 유익한 경험이 될 거에요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPaT76WHqrUV"
      },
      "source": [
        "## 강화 학습 내용 훑어보기\n",
        "\n",
        "강화 학습은 시행착오를 거치며 경험을 통해 적절한 행동 방식을 배워나가는 과정입니다. \"당근과 채찍\" 전략을 생각하면 쉽습니다. 여러분이 좋은 행동을 하면 당근을, 나쁜 행동을 하면 채찍질을 주는 것이 바로 강화 학습입니다. 이 당근과 채찍을 통틀어 강화 학습에서는 \"보상\"이라고 부릅니다. 강화 학습의 가장 중요한 요소 중 하나가 바로 이 보상입니다.\n",
        "\n",
        "![](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/4916/12365.jpeg)\n",
        "\n",
        "강화 학습의 또 다른 중요한 요소는 바로 끊임없이 변화하는 환경의 상태입니다. 어러분이 취하는 행동은 여러분 주변의 환경을 변화시킵니다. 여러분은 이 끊임없이 변화하는 환경 속에서 계속 보상을 최대한 취할 수 있도록 노력해야합니다. 이 과정이 바로 강화 학습의 과정과 많이 닮아있습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94yhPKFYquQw"
      },
      "source": [
        "## 강화 학습 예제 경험하기\n",
        "\n",
        "강화 학습이 어떤 문제를 풀 수 있는지 한번 살펴볼까요?\n",
        "\n",
        "사실 강화 학습은 우리 주변에서 흔히 접할 수 있는 대부분의 문제를 다 풀 수 있는 아주 유용한 기술입니다. 로봇을 조종하는 것부터 시작해서, 자율 주행, 주식 투자, 심지어는 최근에 각광받는 ChatGPT 역시 강화 학습 기술이 사용되었습니다. 우리가 일상 생활에서 상벌의 개념을 생각할 수 있는 거의 모든 문제를 강화 학습으로 풀 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3e7d7NMlQPE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U ray[rllib]\n",
        "!pip install gputil\n",
        "!pip install ipython==7.10.0\n",
        "!pip install moviepy\n",
        "!pip install gymnasium[classic-control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk7kE26_lTjF"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import ray\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "\n",
        "def view_video(filename):\n",
        "    video = f'video/{filename}-episode-0.mp4'\n",
        "    mp4 = open(video,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_OKJ8jCqxuQ"
      },
      "source": [
        "### 환경 정의하기\n",
        "\n",
        "오늘 수업 시간에는 강화 학습에서 가장 단순하고 유명한 예제 중 하나인 \"Mountain Car\" 문제를 직접 풀어볼 것입니다. 이 문제에서 여러분의 목표는 산 정상의 깃발에 도달하는 것입니다.\n",
        "여러분은 오른쪽 또는 왼쪽으로 가속하는 페달을 밟거나, 페달을 밟지 않는 선택을 할 수 있습니다.\n",
        "산 정상까지 올라가려면 페달을 적절히 밟아야 합니다. 하지만 산의 경사가 너무 가파르기 때문에, 아무리 밟아도 중력 때문에 올라가지 못할 수도 있습니다.\n",
        "\n",
        "![](https://www.gymlibrary.dev/_images/mountain_car.gif)\n",
        "\n",
        "먼저, 이 환경에서 여러분이 취할 수 있는 행동은 다음과 같이 표현할 수 있습니다:\n",
        "\n",
        "0: 왼쪽으로 가속하는 페달 밟기\n",
        "\n",
        "1: 페달 밟지 않기\n",
        "\n",
        "2: 오른쪽으로 가속하는 페달 밟기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X25O_F-BlbXG",
        "outputId": "2ea0f639-6eca-4b55-96f0-13d4e461d862"
      },
      "outputs": [],
      "source": [
        "env = gym.make('MountainCar-v0')\n",
        "print('Created env:', env)\n",
        "print('Available actions:', env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "다음으로, 이 환경에서 받을 수 있는 상태에 대한 정보는 $s_{t} = (x_{t}, v_{t})$입니다.\n",
        "즉, 여러분이 받는 상태 정보 $s_{t}$는 $x$축 방향에서의 현재 위치 $x_{t}$와 속도 $v_{t}$입니다.\n",
        "\n",
        "$x$축 방향의 좌표는 $(-1.2, 0.6)$ 사이이고, 속도 $v$는 (-0.07, 0.07) 사이입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XWStLA-ldOd",
        "outputId": "31a371c8-f039-46fb-fa26-11b574704545"
      },
      "outputs": [],
      "source": [
        "state, _ = env.reset()\n",
        "print('Observation space:', env.observation_space)\n",
        "print('The starting state is:', state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "여러분이 어떤 행동을 선택하면, 다음과 같이 환경의 상태가 변화합니다.\n",
        "\n",
        "$v_{t+1} = v_{t} + (\\mathrm{action} - 1) * F - \\cos(3 * x_{t}) * mg$\n",
        "\n",
        "$x_{t+1} = x_{t} + v_{t+1}$\n",
        "\n",
        "여기서 $F$는 페달을 밟을 때 가속하는 힘 $F = 0.001$이고, 중력 $mg = 0.0025$입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYtgu-blgGL",
        "outputId": "825886d5-ccef-4494-a26a-c35b2aa417d3"
      },
      "outputs": [],
      "source": [
        "action = 0\n",
        "state, reward, done, _, _ = env.step(action)\n",
        "print(f\"State: {state}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Done: {done}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCUIOuWYq0Nq"
      },
      "source": [
        "### 인공지능 정책 정의하기\n",
        "\n",
        "강화 학습에서 정책은 인공지능의 두뇌입니다. 인공지능이 변화하는 환경 속에서 어떻게 행동할지를 결정하는 지침으로 생각할 수 있습니다. 가장 단순한 형태의 정책으로는 여러분이 취할 수 있는 행동 중에서 아무거나 (샘플링하여) 고르는 것이 있겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o18p51sq0lv"
      },
      "outputs": [],
      "source": [
        "class RandomPolicy():\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def __call__(self, state):\n",
        "        return self.action_space.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvVYPQrq3h_"
      },
      "source": [
        "### 환경과 상호작용하기\n",
        "\n",
        "정책이 있다면, 여러분은 주어진 상황에서 어떤 행동을 취해야 할지 결정할 수 있습니다. 그럼 이제 본격적으로 환경 속에 들어가서 직접 부딪혀볼까요?\n",
        "\n",
        "여러분이 어떤 행동을 취하면, 환경은 여러분에게 보상을 줍니다. 이 보상은 여러분이 좋은 행동을 했는지, 나쁜 행동을 했는지 알려주는 값입니다. 그리고 여러분이 취한 행동에 따라 환경은 변화하고, 이 변화한 환경을 여러분은 다시 관찰할 수 있습니다. 그리고 여러분은 또다시 (여러분이 관찰한 결과를 토대로) 어떤 행동을 취할지 결정합니다. 강화 학습은 이 과정을 반복하며 진행됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwtH_tezxFYm",
        "outputId": "47c502db-4479-46b3-c6cd-0f38da5f1930"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
        "env = RecordVideo(env, 'video', name_prefix='random-agent')\n",
        "\n",
        "agent = RandomPolicy(env.action_space)\n",
        "state, _ = env.reset()\n",
        "env.start_video_recorder()\n",
        "\n",
        "for t in range(100):\n",
        "    action = agent(state)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "view_video('random-agent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 강화 학습 환경 직접 만져보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 블랙잭"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the Blackjack environment\n",
        "env = gym.make('Blackjack-v1')\n",
        "\n",
        "# Reset the environment\n",
        "state = env.reset()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # Choose a random action\n",
        "    action = ???\n",
        "\n",
        "    # Take a step in the environment\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "    # Print the current state, reward, and whether the episode is done\n",
        "    print('State:', state)\n",
        "    print('Reward:', reward)\n",
        "    print('Done:', done)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tic-Tac-Toe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(3, 3), dtype=int)\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3), dtype=int)\n",
        "        self.current_player = 1\n",
        "        return self.board\n",
        "\n",
        "    def step(self, action):\n",
        "        row = action // 3\n",
        "        col = action % 3\n",
        "\n",
        "        if self.board[row, col] != 0:\n",
        "            return self.board, -1, False, False, {}\n",
        "\n",
        "        self.board[row, col] = self.current_player\n",
        "\n",
        "        if self._check_winner(self.current_player):\n",
        "            return self.board, 1, True, False, {}\n",
        "\n",
        "        if np.count_nonzero(self.board) == 9:\n",
        "            return self.board, 0, True, False, {}\n",
        "\n",
        "        self.current_player = -self.current_player\n",
        "        return self.board, 0, False, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(self.board)\n",
        "\n",
        "    def _check_winner(self, player):\n",
        "        for i in range(3):\n",
        "            if np.all(self.board[i, :] == player) or np.all(self.board[:, i] == player):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == player) or np.all(np.diag(np.fliplr(self.board)) == player):\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = TicTacToeEnv()\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = ?????\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
